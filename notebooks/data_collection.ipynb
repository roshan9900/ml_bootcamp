{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c07a4d8f-5a8c-4545-9351-eb5920a92235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting BeautifulSoup\n",
      "  Using cached BeautifulSoup-3.2.2.tar.gz (32 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Getting requirements to build wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [22 lines of output]\n",
      "      Traceback (most recent call last):\n",
      "        File \"C:\\Users\\hp\\Downloads\\mlbootcamp\\venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 389, in <module>\n",
      "          main()\n",
      "        File \"C:\\Users\\hp\\Downloads\\mlbootcamp\\venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 373, in main\n",
      "          json_out[\"return_val\"] = hook(**hook_input[\"kwargs\"])\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\hp\\Downloads\\mlbootcamp\\venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 143, in get_requires_for_build_wheel\n",
      "          return hook(config_settings)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\hp\\AppData\\Local\\Temp\\pip-build-env-zs4_0vx8\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 334, in get_requires_for_build_wheel\n",
      "          return self._get_build_requires(config_settings, requirements=[])\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\hp\\AppData\\Local\\Temp\\pip-build-env-zs4_0vx8\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 304, in _get_build_requires\n",
      "          self.run_setup()\n",
      "        File \"C:\\Users\\hp\\AppData\\Local\\Temp\\pip-build-env-zs4_0vx8\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 522, in run_setup\n",
      "          super().run_setup(setup_script=setup_script)\n",
      "        File \"C:\\Users\\hp\\AppData\\Local\\Temp\\pip-build-env-zs4_0vx8\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 320, in run_setup\n",
      "          exec(code, locals())\n",
      "        File \"<string>\", line 3\n",
      "          \"You're trying to run a very old release of Beautiful Soup under Python 3. This will not work.\"<>\"Please use Beautiful Soup 4, available through the pip package 'beautifulsoup4'.\"\n",
      "                                                                                                         ^^\n",
      "      SyntaxError: invalid syntax\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "× Getting requirements to build wheel did not run successfully.\n",
      "│ exit code: 1\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bca3a341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests)\n",
      "  Using cached charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl (102 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Installing collected packages: urllib3, idna, charset-normalizer, certifi, requests\n",
      "Successfully installed certifi-2025.1.31 charset-normalizer-3.4.1 idna-3.10 requests-2.32.3 urllib3-2.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c348d40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Collecting beautifulsoup4 (from bs4)\n",
      "  Downloading beautifulsoup4-4.13.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->bs4)\n",
      "  Downloading soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typing-extensions>=4.0.0 (from beautifulsoup4->bs4)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Downloading beautifulsoup4-4.13.3-py3-none-any.whl (186 kB)\n",
      "Downloading soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: typing-extensions, soupsieve, beautifulsoup4, bs4\n",
      "Successfully installed beautifulsoup4-4.13.3 bs4-0.0.2 soupsieve-2.6 typing-extensions-4.12.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a43da091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Using cached numpy-2.2.3-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp\\downloads\\mlbootcamp\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\downloads\\mlbootcamp\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "Using cached numpy-2.2.3-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "Using cached pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "Using cached tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-2.2.3 pandas-2.2.3 pytz-2025.1 tzdata-2025.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b48ee00-cdff-4a3a-aad6-34efdd70466a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Set the URL for the search results page\\nurl = \"https://www.flipkart.com/mobiles/pr?sid=tyy%2C4io&p%5B%5D=facets.brand%255B%255D%3DMOTOROLA&param=19873&ctx=eyJjYXJkQ29udGV4dCI6eyJhdHRyaWJ1dGVzIjp7InRpdGxlIjp7Im11bHRpVmFsdWVkQXR0cmlidXRlIjp7ImtleSI6InRpdGxlIiwiaW5mZXJlbmNlVHlwZSI6IlRJVExFIiwidmFsdWVzIjpbIk1vdG9yb2xhIHNtYXJ0cGhvbmVzIl0sInZhbHVlVHlwZSI6Ik1VTFRJX1ZBTFVFRCJ9fX19fQ%3D%3D&wid=21.productCard.PMU_V2_16\"\\n\\n# Add headers to mimic a real browser request\\nheaders = {\\n    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\\n}\\n\\n# Send a GET request\\nresponse = requests.get(url, headers=headers)\\n\\n# Parse the HTML content\\nsoup = BeautifulSoup(response.content, \"html.parser\")\\n\\n# Extract product details\\nproducts = soup.find_all(\"div\", class_=\"cPHDOP\")\\n\\n# Loop through products and extract data\\nfor product in products:\\n    title = product.find(\"div\", class_=\"KzDlHZ\")\\n    price = product.find(\"div\", class_=\"Nx9bqj _4b5DiR\")\\n    rating = product.find(\"div\", class_=\"Wphh3N\")\\n    if title and price:\\n        print(f\"Product: {title.text}\")\\n        print(f\"Price: {price.text}\")\\n        print(f\"Rating: {rating.text if rating else \\'No rating\\'}\")\\n        print(\"-\" * 50)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Set the URL for the search results page\n",
    "url = \"https://www.flipkart.com/mobiles/pr?sid=tyy%2C4io&p%5B%5D=facets.brand%255B%255D%3DMOTOROLA&param=19873&ctx=eyJjYXJkQ29udGV4dCI6eyJhdHRyaWJ1dGVzIjp7InRpdGxlIjp7Im11bHRpVmFsdWVkQXR0cmlidXRlIjp7ImtleSI6InRpdGxlIiwiaW5mZXJlbmNlVHlwZSI6IlRJVExFIiwidmFsdWVzIjpbIk1vdG9yb2xhIHNtYXJ0cGhvbmVzIl0sInZhbHVlVHlwZSI6Ik1VTFRJX1ZBTFVFRCJ9fX19fQ%3D%3D&wid=21.productCard.PMU_V2_16\"\n",
    "\n",
    "# Add headers to mimic a real browser request\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "}\n",
    "\n",
    "# Send a GET request\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Extract product details\n",
    "products = soup.find_all(\"div\", class_=\"cPHDOP\")\n",
    "\n",
    "# Loop through products and extract data\n",
    "for product in products:\n",
    "    title = product.find(\"div\", class_=\"KzDlHZ\")\n",
    "    price = product.find(\"div\", class_=\"Nx9bqj _4b5DiR\")\n",
    "    rating = product.find(\"div\", class_=\"Wphh3N\")\n",
    "    if title and price:\n",
    "        print(f\"Product: {title.text}\")\n",
    "        print(f\"Price: {price.text}\")\n",
    "        print(f\"Rating: {rating.text if rating else 'No rating'}\")\n",
    "        print(\"-\" * 50)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c778a897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(r'C:\\Users\\hp\\Downloads\\mlbootcamp\\data\\raw_data',exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a083fa21-2298-488c-be1b-3715f940c3f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import requests\\nfrom bs4 import BeautifulSoup\\nimport pandas as pd\\n\\ndata = []\\n\\nfor i in range(1, 440):  \\n    url = f\\'https://www.flipkart.com/mobiles/pr?sid=tyy%2C4io&param=19873&ctx=eyJjYXJkQ29udGV4dCI6eyJhdHRyaWJ1dGVzIjp7InRpdGxlIjp7Im11bHRpVmFsdWVkQXR0cmlidXRlIjp7ImtleSI6InRpdGxlIiwiaW5mZXJlbmNlVHlwZSI6IlRJVExFIiwidmFsdWVzIjpbIk1vdG9yb2xhIHNtYXJ0cGhvbmVzIl0sInZhbHVlVHlwZSI6Ik1VTFRJX1ZBTFVFRCJ9fX19fQ%3D%3D&wid=21.productCard.PMU_V2_16&sort=popularity&page={i}\\'\\n\\n    headers = {\\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\\n    }\\n\\n    response = requests.get(url, headers=headers)\\n\\n    if response.status_code != 200:\\n        print(f\"Failed to retrieve page {i}\")\\n        continue\\n\\n    soup = BeautifulSoup(response.content, \"html.parser\")\\n\\n    products = soup.find_all(\"div\", class_=\"cPHDOP\")  \\n    for product in products:\\n        title = product.find(\"div\", class_=\"KzDlHZ\")\\n        price = product.find(\"div\", class_=\"Nx9bqj _4b5DiR\")\\n        actual_price = product.find(\"div\", class_=\"yRaY8j ZYYwLA\")\\n        rating = product.find(\"div\", class_=\"_5OesEi\")\\n        storage = product.find(\"div\", class_=\"_6NESgJ\")\\n\\n        if title and price:\\n            product_data = {\\n                \"Product\": title.text.strip(),\\n                \"Price\": price.text.strip(),\\n                \"Actual Price\": actual_price.text.strip() if actual_price else \"N/A\",\\n                \"Rating\": rating.text.strip() if rating else \"No rating\",\\n                \"Storage\": storage.text.strip() if storage else \"N/A\"\\n            }\\n            data.append(product_data)\\n\\n    print(f\"Page {i} scraped successfully\")\\n\\nproduct_df = pd.DataFrame(data)\\n\\nproduct_df.to_csv(\"flipkart_motorola_products.csv\", index=False)\\n\\nprint(f\"Scraped {product_df.shape[0]} products.\")\\n'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "data = []\n",
    "\n",
    "for i in range(1, 440):  \n",
    "    url = f'https://www.flipkart.com/mobiles/pr?sid=tyy%2C4io&param=19873&ctx=eyJjYXJkQ29udGV4dCI6eyJhdHRyaWJ1dGVzIjp7InRpdGxlIjp7Im11bHRpVmFsdWVkQXR0cmlidXRlIjp7ImtleSI6InRpdGxlIiwiaW5mZXJlbmNlVHlwZSI6IlRJVExFIiwidmFsdWVzIjpbIk1vdG9yb2xhIHNtYXJ0cGhvbmVzIl0sInZhbHVlVHlwZSI6Ik1VTFRJX1ZBTFVFRCJ9fX19fQ%3D%3D&wid=21.productCard.PMU_V2_16&sort=popularity&page={i}'\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve page {i}\")\n",
    "        continue\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    products = soup.find_all(\"div\", class_=\"cPHDOP\")  \n",
    "    for product in products:\n",
    "        title = product.find(\"div\", class_=\"KzDlHZ\")\n",
    "        price = product.find(\"div\", class_=\"Nx9bqj _4b5DiR\")\n",
    "        actual_price = product.find(\"div\", class_=\"yRaY8j ZYYwLA\")\n",
    "        rating = product.find(\"div\", class_=\"_5OesEi\")\n",
    "        storage = product.find(\"div\", class_=\"_6NESgJ\")\n",
    "\n",
    "        if title and price:\n",
    "            product_data = {\n",
    "                \"Product\": title.text.strip(),\n",
    "                \"Price\": price.text.strip(),\n",
    "                \"Actual Price\": actual_price.text.strip() if actual_price else \"N/A\",\n",
    "                \"Rating\": rating.text.strip() if rating else \"No rating\",\n",
    "                \"Storage\": storage.text.strip() if storage else \"N/A\"\n",
    "            }\n",
    "            data.append(product_data)\n",
    "\n",
    "    print(f\"Page {i} scraped successfully\")\n",
    "\n",
    "product_df = pd.DataFrame(data)\n",
    "\n",
    "product_df.to_csv(r\"C:\\Users\\hp\\Downloads\\mlbootcamp\\data\\raw_data\\flipkart_motorola_products.csv\", index=False)\n",
    "\n",
    "print(f\"Scraped {product_df.shape[0]} products.\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "386520f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import requests\\nfrom bs4 import BeautifulSoup\\nimport pandas as pd\\nimport time\\nimport random\\n\\n# Initialize an empty list to store product data\\ndata = []\\n\\n# Set the number of pages (modify if needed)\\ntotal_pages = 440  \\n\\n# User-Agent list for rotation\\nuser_agents = [\\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\\n    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\\n    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36\"\\n]\\n\\n# Loop through multiple pages\\nfor i in range(1, total_pages + 1):\\n    url = f\\'https://www.flipkart.com/mobiles/pr?sid=tyy%2C4io&sort=popularity&page={i}\\'\\n\\n    # Rotate user agents\\n    headers = {\"User-Agent\": random.choice(user_agents)}\\n\\n    try:\\n        response = requests.get(url, headers=headers, timeout=10)\\n\\n        # Check if the request was successful\\n        if response.status_code != 200:\\n            print(f\"Failed to retrieve page {i}, Status Code: {response.status_code}\")\\n            continue\\n\\n        # Parse the HTML content\\n        soup = BeautifulSoup(response.content, \"html.parser\")\\n\\n        # Extract product details\\n        products = soup.find_all(\"div\", class_=\"cPHDOP\")  # Verify correct class\\n\\n        for product in products:\\n            title = product.find(\"div\", class_=\"KzDlHZ\")\\n            price = product.find(\"div\", class_=\"Nx9bqj _4b5DiR\")\\n            actual_price = product.find(\"div\", class_=\"yRaY8j ZYYwLA\")\\n            rating = product.find(\"div\", class_=\"_5OesEi\")\\n            storage = product.find(\"li\", class_=\"_6NESgJ\")\\n            \"\"\"\\n                products = soup.find_all(\"div\", class_=\"cPHDOP\")  \\n                for product in products:\\n                    title = product.find(\"div\", class_=\"KzDlHZ\")\\n                    price = product.find(\"div\", class_=\"Nx9bqj _4b5DiR\")\\n                    actual_price = product.find(\"div\", class_=\"yRaY8j ZYYwLA\")\\n                    rating = product.find(\"div\", class_=\"_5OesEi\")\\n                    storage = product.find(\"div\", class_=\"_6NESgJ\")\\n            \"\"\"\\n            if title and price:\\n                product_data = {\\n                    \"Product\": title.text.strip(),\\n                    \"Price\": price.text.strip(),\\n                    \"Actual Price\": actual_price.text.strip() if actual_price else \"N/A\",\\n                    \"Rating\": rating.text.strip() if rating else \"No rating\",\\n                    \"Storage\": storage.text.strip() if storage else \"N/A\"\\n                }\\n                data.append(product_data)\\n\\n        print(f\"Page {i} scraped successfully, Total Products: {len(data)}\")\\n\\n        # Add a delay to avoid being blocked\\n        time.sleep(random.uniform(2, 5))\\n\\n    except Exception as e:\\n        print(f\"Error on page {i}: {e}\")\\n\\n# Create a DataFrame\\nproduct_df = pd.DataFrame(data)\\n\\n# Save the DataFrame to a CSV file\\nproduct_df.to_csv(\"flipkart_motorola_products1111.csv\", index=False)\\n\\n# Display final DataFrame shape\\nprint(f\"Total Products Scraped: {product_df.shape[0]}\")\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Initialize an empty list to store product data\n",
    "data = []\n",
    "\n",
    "# Set the number of pages (modify if needed)\n",
    "total_pages = 440  \n",
    "\n",
    "# User-Agent list for rotation\n",
    "user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36\"\n",
    "]\n",
    "\n",
    "# Loop through multiple pages\n",
    "for i in range(1, total_pages + 1):\n",
    "    url = f'https://www.flipkart.com/mobiles/pr?sid=tyy%2C4io&sort=popularity&page={i}'\n",
    "\n",
    "    # Rotate user agents\n",
    "    headers = {\"User-Agent\": random.choice(user_agents)}\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve page {i}, Status Code: {response.status_code}\")\n",
    "            continue\n",
    "\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Extract product details\n",
    "        products = soup.find_all(\"div\", class_=\"cPHDOP\")  # Verify correct class\n",
    "\n",
    "        for product in products:\n",
    "            title = product.find(\"div\", class_=\"KzDlHZ\")\n",
    "            price = product.find(\"div\", class_=\"Nx9bqj _4b5DiR\")\n",
    "            actual_price = product.find(\"div\", class_=\"yRaY8j ZYYwLA\")\n",
    "            rating = product.find(\"div\", class_=\"_5OesEi\")\n",
    "            storage = product.find(\"li\", class_=\"_6NESgJ\")\n",
    "            \"\"\"\n",
    "                products = soup.find_all(\"div\", class_=\"cPHDOP\")  \n",
    "                for product in products:\n",
    "                    title = product.find(\"div\", class_=\"KzDlHZ\")\n",
    "                    price = product.find(\"div\", class_=\"Nx9bqj _4b5DiR\")\n",
    "                    actual_price = product.find(\"div\", class_=\"yRaY8j ZYYwLA\")\n",
    "                    rating = product.find(\"div\", class_=\"_5OesEi\")\n",
    "                    storage = product.find(\"div\", class_=\"_6NESgJ\")\n",
    "            \"\"\"\n",
    "            if title and price:\n",
    "                product_data = {\n",
    "                    \"Product\": title.text.strip(),\n",
    "                    \"Price\": price.text.strip(),\n",
    "                    \"Actual Price\": actual_price.text.strip() if actual_price else \"N/A\",\n",
    "                    \"Rating\": rating.text.strip() if rating else \"No rating\",\n",
    "                    \"Storage\": storage.text.strip() if storage else \"N/A\"\n",
    "                }\n",
    "                data.append(product_data)\n",
    "\n",
    "        print(f\"Page {i} scraped successfully, Total Products: {len(data)}\")\n",
    "\n",
    "        # Add a delay to avoid being blocked\n",
    "        time.sleep(random.uniform(2, 5))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error on page {i}: {e}\")\n",
    "\n",
    "# Create a DataFrame\n",
    "product_df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "product_df.to_csv(\"flipkart_motorola_products1111.csv\", index=False)\n",
    "\n",
    "# Display final DataFrame shape\n",
    "print(f\"Total Products Scraped: {product_df.shape[0]}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87e90cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
